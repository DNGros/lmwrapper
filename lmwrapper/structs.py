from dataclasses import dataclass
import statistics
from typing import List, Any, Union, Tuple


LM_CHAT_DIALOG_COERCIBLE_TYPES = Union[
    str,
    List[Union["LmChatTurn", Tuple[str, str], dict, str]],
    "LmChatDialog",
]  # Defines a set of types that can be converted into a LmChatDialog


@dataclass(frozen=True)
class LmPrompt:
    text: Union[str, LM_CHAT_DIALOG_COERCIBLE_TYPES]
    max_tokens: int
    stop: List[str] = None
    logprobs: int = None
    temperature: float = 1.0
    top_p: float = 0.9
    presence_penalty: float = 0.0
    num_completions: int = 1
    cache: bool = None  # Use the default of the predictor
    """Whether to attempt to cache the model output. This overrides any default
    settings of the model. This can be useful in saving computation but means 
    sampling might not work as expected."""
    echo: bool = False
    """Whether to echo back the original prompt. Also allows you to get the
    probability of the prompt under the model"""

    def is_text_a_chat(self) -> bool:
        return isinstance(self.text, list)

    def get_text_as_chat(self) -> "LmChatDialog":
        return LmChatDialog(self.text)

@dataclass
class LmChatTurn:
    role: str
    """The role of the messages author. For OpenAI: one of system, user, assistant, or function."""
    content: str
    """The contents of the message. 
    content is required for all messages except assistant messages with function calls."""
    name: str = None
    """The name of the author of this message. name is required if role 
    is function, and it should be the name of the function whose response is 
    in the content. May contain a-z, A-Z, 0-9, and underscores, 
    with a maximum length of 64 characters."""
    function_call: str = None
    """The name and arguments of a function that should be called, as generated by the model."""


class LmChatDialog(list[LmChatTurn]):
    def __init__(self, values: LM_CHAT_DIALOG_COERCIBLE_TYPES):
        out = []
        current_role = "user"
        text_list = values if isinstance(values, list) else [values]
        for turn in text_list:
            match turn:
                case str(text):
                    out.append(LmChatTurn(role=current_role, content=text))
                    current_role = "user" if current_role == "system" else "system"
                case (str(role), str(content)):
                    out.append(LmChatTurn(role=role, content=content))
                    current_role = "user" if role == "system" else "system"
                case dict(turn):
                    out.append(LmChatTurn(**turn))
                    current_role = "user" if turn['role'] == "system" else "system"
                case LmChatTurn(role=role, content=content):
                    out.append(turn)
                    current_role = "user" if role == "system" else "system"
                case _:
                    raise ValueError(f"Invalid type for text: {type(turn)}. "
                                     f"It should be a tuple of strings (role, content) or a LmChatTurn.")
        super().__init__(out)

    def as_dicts(self) -> List[dict]:
        return [
            {
                k: v
                for k, v in chat_turn.__dict__.items()
                if v is not None
            }
            for chat_turn in self
        ]


@dataclass
class LmPrediction:
    completion_text: str
    prompt: LmPrompt
    metad: Any

    @property
    def completion_tokens(self):
        raise NotImplemented("This version of prediction does not support completion tokens")

    @property
    def completion_token_offsets(self):
        raise NotImplemented("This version of prediction does not support completion token offsets")

    @property
    def completion_logprobs(self) -> List[float]:
        raise NotImplemented("This version of prediction does not support completion logprobs")

    @property
    def prompt_tokens(self):
        raise NotImplemented("This version of prediction does not support prompt tokens")

    @property
    def prompt_token_offsets(self):
        raise NotImplemented("This version of prediction does not support prompt token offsets")

    @property
    def prompt_logprobs(self):
        raise NotImplemented("This version of prediction does not support prompt logprobs")

    def get_full_text(self):
        return self.prompt.text + self.completion_text

    def completion_mean_logprob(self):
        return statistics.mean(self.completion_logprobs)
